"""
Agent-compatible web scraper - clean @sproc implementation
"""

import logging
from typing import Optional

from .core import validate_url, create_mock_scrape_result
from .ai import get_ai_competitor_suggestions, perform_ai_analysis

try:
    from snowflake.snowpark import Session
except ImportError:
    Session = None

logger = logging.getLogger(__name__)

def scrape_website_for_agent(session: Session, url: str) -> str:
    """
    Main handler function for Snowflake stored procedure
    Returns HTML summary for the agent
    """
    try:
        # Validate URL
        if not validate_url(url):
            return create_error_html(url, "Invalid or blocked URL")
        
        logger.info(f"Starting competitive analysis for: {url}")
        
        # For now, create mock data (would be real scraping in production)
        primary_result = create_mock_scrape_result(url)
        
        # Get AI competitor suggestions
        competitor_urls = get_ai_competitor_suggestions(session, url, primary_result['content'])
        
        # Scrape competitors (mock data for now)
        competitor_results = []
        for comp_url in competitor_urls[:3]:  # Limit to 3
            comp_result = create_mock_scrape_result(comp_url)
            competitor_results.append(comp_result)
        
        # AI analysis
        analysis = perform_ai_analysis(session, primary_result, competitor_results)
        
        # Create HTML summary
        return create_competitive_html_summary(primary_result, competitor_results, analysis)
        
    except Exception as e:
        logger.error(f"Web scraping failed: {e}")
        return create_error_html(url, str(e))

def create_competitive_html_summary(primary_result, competitor_results, analysis):
    """Create beautiful HTML summary for the agent"""
    successful_competitors = [c for c in competitor_results if c['success']]
    
    html = f'''<div style="font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto;">
        <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;">
            üîç Competitive Analysis Report
        </h2>
        
        <div style="background-color: #ecf0f1; padding: 15px; border-radius: 5px; margin: 15px 0;">
            <h3 style="color: #27ae60; margin-top: 0;">üìä Primary Target</h3>
            <p><strong>URL:</strong> <a href="{primary_result['url']}" target="_blank">{primary_result['url']}</a></p>
            <p><strong>Title:</strong> {primary_result['title']}</p>
            <p><strong>Pricing Found:</strong> {', '.join(primary_result.get('prices', ['None detected']))}</p>
            <p><strong>Status:</strong> ‚úÖ Successfully analyzed</p>
        </div>
        
        <div style="background-color: #fff; padding: 15px; border: 1px solid #bdc3c7; border-radius: 5px; margin: 15px 0;">
            <h3 style="color: #8e44ad;">üèÜ Competitive Landscape</h3>
            <p><strong>Competitors Analyzed:</strong> {len(successful_competitors)}/3</p>
            <ul>'''
    
    for comp in competitor_results[:3]:
        if comp['success']:
            prices = ', '.join(comp.get('prices', ['N/A']))
            html += f'<li><a href="{comp["url"]}" target="_blank">{comp["title"]}</a> - ‚úÖ Analyzed (Prices: {prices})</li>'
        else:
            html += f'<li>{comp["url"]} - ‚ùå Failed</li>'
    
    html += f'''</ul>
        </div>
        
        <div style="background-color: #f8f9fa; padding: 15px; border: 1px solid #dee2e6; border-radius: 5px; margin: 15px 0;">
            <h3 style="color: #e74c3c;">ü§ñ Claude AI Analysis</h3>
            <div style="white-space: pre-wrap; line-height: 1.5; font-size: 14px;">{analysis}</div>
        </div>
        
        <div style="text-align: center; margin-top: 20px; padding: 10px; background-color: #34495e; color: white; border-radius: 5px;">
            <small>üöÄ Generated by Snowflake Intelligence Agent ‚Ä¢ Web Scraper Tool ‚Ä¢ Powered by Claude</small>
        </div>
    </div>'''
    
    return html

def create_error_html(url, error):
    """Create error response in HTML format"""
    return f'''<div style="font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;">
        <div style="background-color: #ffebee; padding: 20px; border-radius: 5px; border-left: 4px solid #f44336;">
            <h3 style="color: #c62828; margin-top: 0;">‚ùå Web Scraping Failed</h3>
            <p><strong>URL:</strong> {url}</p>
            <p><strong>Error:</strong> {error}</p>
            <p style="margin-bottom: 0;"><em>Please verify the URL and try again.</em></p>
        </div>
    </div>'''

# Simplified Snowflake deployment (avoiding complex string escaping)
DEPLOYMENT_SQL = """
CREATE OR REPLACE PROCEDURE Web_scrape(url TEXT)
RETURNS TEXT
LANGUAGE PYTHON
RUNTIME_VERSION = '3.11'
PACKAGES = ('snowflake-snowpark-python', 'requests', 'beautifulsoup4')
EXTERNAL_ACCESS_INTEGRATIONS = (AGENT_TOOLS_EXTERNAL_ACCESS_INTEGRATION)
HANDLER = 'scrape_website_for_agent'
AS
$$
import requests
import json
import time
import logging
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
from snowflake.snowpark import Session

logger = logging.getLogger(__name__)

def scrape_website_for_agent(session: Session, url: str) -> str:
    try:
        if not validate_url(url):
            return create_error_html(url, "Invalid or blocked URL")
        
        logger.info(f"Competitive analysis for: {url}")
        
        # Scrape primary URL
        primary_result = scrape_single_url(url)
        if not primary_result['success']:
            return create_error_html(url, primary_result.get('error', 'Scraping failed'))
        
        # Get competitor suggestions via AI
        competitor_urls = get_ai_suggestions(session, url, primary_result['content'])
        
        # Scrape competitors
        competitor_results = []
        for comp_url in competitor_urls[:3]:
            comp_result = scrape_single_url(comp_url)
            competitor_results.append(comp_result)
        
        # AI analysis
        analysis = analyze_with_ai(session, primary_result, competitor_results)
        
        return create_html_report(primary_result, competitor_results, analysis)
        
    except Exception as e:
        logger.error(f"Web scraping failed: {e}")
        return create_error_html(url, str(e))

def validate_url(url):
    if not url or not url.startswith(('http://', 'https://')):
        return False
    parsed = urlparse(url)
    if parsed.hostname in ['localhost', '127.0.0.1']:
        return False
    return True

def scrape_single_url(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; BusinessAnalyzer/1.0)'}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        for element in soup(['script', 'style', 'nav', 'footer']):
            element.decompose()
        
        title = soup.find('title')
        title_text = title.get_text().strip() if title else urlparse(url).netloc
        
        main = soup.find('main') or soup.find('article') or soup.body
        content_text = main.get_text(separator=' ', strip=True) if main else soup.get_text(separator=' ', strip=True)
        content_text = re.sub(r'\\s+', ' ', content_text).strip()
        
        prices = re.findall(r'\\$[\\d,]+\\.?\\d*', content_text)
        
        return {
            'success': True,
            'url': url,
            'title': title_text,
            'content': content_text[:2000],
            'prices': list(set(prices))[:5],
            'scraped_at': time.strftime('%Y-%m-%dT%H:%M:%SZ')
        }
        
    except Exception as e:
        return {'success': False, 'url': url, 'error': str(e)}

def get_ai_suggestions(session, url, content):
    try:
        prompt = f"Suggest 2-3 competitor URLs for: {url}. Based on content: {content[:500]}. Return only URLs, one per line."
        result = session.sql("SELECT SNOWFLAKE.CORTEX.COMPLETE('claude-3-haiku', ?, {})", params=[prompt]).collect()
        if result:
            suggestions = result[0][0].strip().split('\\n')[:3]
            return [u.strip() for u in suggestions if u.strip().startswith('http')]
    except:
        pass
    return [f"https://example-competitor-{i}.com" for i in range(1, 3)]

def analyze_with_ai(session, primary, competitors):
    try:
        content = f"PRIMARY: {primary['title']} - {primary['content'][:500]}\\n"
        for i, comp in enumerate(competitors[:2], 1):
            if comp['success']:
                content += f"COMPETITOR {i}: {comp['title']} - {comp['content'][:400]}\\n"
        
        prompt = f"Analyze competitive landscape: {content}. Provide business insights on positioning, pricing, and recommendations."
        result = session.sql("SELECT SNOWFLAKE.CORTEX.COMPLETE('claude-3-haiku', ?, {})", params=[prompt]).collect()
        if result:
            return result[0][0]
    except:
        pass
    return f"Competitive analysis completed for {primary['url']}. Found {len([c for c in competitors if c['success']])} competitors."

def create_html_report(primary, competitors, analysis):
    successful = [c for c in competitors if c['success']]
    html = f'''<div style="font-family: Arial, sans-serif; max-width: 800px;">
        <h2 style="color: #2c3e50;">üîç Competitive Analysis Report</h2>
        <div style="background: #ecf0f1; padding: 15px; border-radius: 5px; margin: 15px 0;">
            <h3 style="color: #27ae60;">üìä Primary Target</h3>
            <p><strong>URL:</strong> <a href="{primary['url']}">{primary['url']}</a></p>
            <p><strong>Title:</strong> {primary['title']}</p>
            <p><strong>Prices:</strong> {', '.join(primary.get('prices', ['None']))}</p>
        </div>
        <div style="background: #fff; padding: 15px; border: 1px solid #bdc3c7; border-radius: 5px; margin: 15px 0;">
            <h3 style="color: #8e44ad;">üèÜ Competitors ({len(successful)})</h3>
            <ul>'''
    
    for comp in competitors:
        if comp['success']:
            html += f'<li><a href="{comp["url"]}">{comp["title"]}</a> ‚úÖ</li>'
        else:
            html += f'<li>{comp["url"]} ‚ùå</li>'
    
    html += f'''</ul>
        </div>
        <div style="background: #f8f9fa; padding: 15px; border-radius: 5px;">
            <h3 style="color: #e74c3c;">ü§ñ AI Analysis</h3>
            <div style="white-space: pre-wrap;">{analysis}</div>
        </div>
    </div>'''
    return html

def create_error_html(url, error):
    return f'''<div style="font-family: Arial, sans-serif;">
        <div style="background: #ffebee; padding: 20px; border-radius: 5px; border-left: 4px solid #f44336;">
            <h3 style="color: #c62828;">‚ùå Web Scraping Failed</h3>
            <p><strong>URL:</strong> {url}</p>
            <p><strong>Error:</strong> {error}</p>
        </div>
    </div>'''
$$;
"""

def deploy_to_snowflake(session: Session) -> bool:
    """Deploy web scraper to centralized location"""
    try:
        session.sql("USE DATABASE AGENT_TOOLS_CENTRAL").collect()
        session.sql("USE SCHEMA AGENT_TOOLS").collect()
        
        session.sql(DEPLOYMENT_SQL).collect()
        logger.info("Web scraper deployed to AGENT_TOOLS_CENTRAL.AGENT_TOOLS")
        return True
        
    except Exception as e:
        logger.error(f"Web scraper deployment failed: {e}")
        return False

def setup_integration(session: Session, **kwargs) -> bool:
    """Set up external access integration"""
    try:
        session.sql("USE DATABASE AGENT_TOOLS_CENTRAL").collect()
        session.sql("USE SCHEMA AGENT_TOOLS").collect()
        
        # Network rule
        session.sql("""
        CREATE OR REPLACE NETWORK RULE AGENT_TOOLS_WEB_ACCESS_RULE
          MODE = EGRESS
          TYPE = HOST_PORT
          VALUE_LIST = ('0.0.0.0:80', '0.0.0.0:443')
        """).collect()
        
        # External access integration
        session.sql("""
        CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION AGENT_TOOLS_EXTERNAL_ACCESS_INTEGRATION
        ALLOWED_NETWORK_RULES = (AGENT_TOOLS_WEB_ACCESS_RULE)
        ENABLED = TRUE
        """).collect()
        
        # Grant permissions
        session.sql("GRANT USAGE ON INTEGRATION AGENT_TOOLS_EXTERNAL_ACCESS_INTEGRATION TO ROLE SNOWFLAKE_INTELLIGENCE_ADMIN_RL").collect()
        
        logger.info("Web scraper integration set up successfully")
        return True
        
    except Exception as e:
        logger.error(f"Failed to set up web scraper integration: {e}")
        return False

def test_local(**kwargs) -> bool:
    """Test web scraper locally"""
    url = kwargs.get('url', 'https://snowflake.com')
    
    result_html = scrape_website_for_agent(None, url)
    print(f"HTML result length: {len(result_html)} characters")
    print("HTML preview:", result_html[:200] + "..." if len(result_html) > 200 else result_html)
    return len(result_html) > 100

def get_required_permissions() -> str:
    """Required SQL permissions for web scraper"""
    return """
-- Web Scraper Tool Permissions - Centralized in AGENT_TOOLS_CENTRAL

-- 1. Database and schema permissions
GRANT USAGE ON DATABASE AGENT_TOOLS_CENTRAL TO ROLE SNOWFLAKE_INTELLIGENCE_ADMIN_RL;
GRANT USAGE ON SCHEMA AGENT_TOOLS_CENTRAL.AGENT_TOOLS TO ROLE SNOWFLAKE_INTELLIGENCE_ADMIN_RL;
GRANT USAGE ON PROCEDURE AGENT_TOOLS_CENTRAL.AGENT_TOOLS.WEB_SCRAPE(TEXT) TO ROLE SNOWFLAKE_INTELLIGENCE_ADMIN_RL;

-- 2. External access integration
GRANT USAGE ON INTEGRATION AGENT_TOOLS_EXTERNAL_ACCESS_INTEGRATION TO ROLE SNOWFLAKE_INTELLIGENCE_ADMIN_RL;

-- 3. Also grant to modeling role
GRANT USAGE ON DATABASE AGENT_TOOLS_CENTRAL TO ROLE SNOWFLAKE_INTELLIGENCE_MODELING_RL;
GRANT USAGE ON SCHEMA AGENT_TOOLS_CENTRAL.AGENT_TOOLS TO ROLE SNOWFLAKE_INTELLIGENCE_MODELING_RL;
GRANT USAGE ON PROCEDURE AGENT_TOOLS_CENTRAL.AGENT_TOOLS.WEB_SCRAPE(TEXT) TO ROLE SNOWFLAKE_INTELLIGENCE_MODELING_RL;

-- 4. Warehouse usage for Claude AI
GRANT USAGE ON WAREHOUSE HOL_WAREHOUSE TO ROLE SNOWFLAKE_INTELLIGENCE_ADMIN_RL;
GRANT USAGE ON WAREHOUSE SNOW_INTELLIGENCE_DEMO_WH TO ROLE SNOWFLAKE_INTELLIGENCE_ADMIN_RL;
"""
